@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}



#REsults section Rouge citation
@inproceedings{lin-2004-rouge,
  title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
  author = "Lin, Chin-Yew",
  booktitle = "Text Summarization Branches Out",
  month = jul,
  year = "2004",
  address = "Barcelona, Spain",
  publisher = "Association for Computational Linguistics",
  url = "https://www.aclweb.org/anthology/W04-1013",
  pages = "74--81",
}


Bert model explanation for method
@article{liu2019text,
  title={Text summarization with pretrained encoders},
  author={Liu, Yang and Lapata, Mirella},
  journal={arXiv preprint arXiv:1908.08345},
  year={2019}
}


#Method explanation
@misc{miller2019leveraging,
  title={Leveraging BERT for Extractive Text Summarization on Lectures}, 
  author={Derek Miller},
  year={2019},
  eprint={1906.04165},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017}
}

#XLnet explanation
@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}


##Discussion
@inproceedings{mass2020ad,
  title={Ad-hoc Document Retrieval Using Weak-Supervision with BERT and GPT2},
  author={Mass, Yosi and Roitman, Haggai},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={4191--4197},
  year={2020}
}